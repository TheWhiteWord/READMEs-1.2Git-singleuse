const ollama = require('ollama');

/**
 * Sends a message to the LLM and receives a response.
 * @param {string} message - The message to send to the LLM.
 * @returns {Promise<string>} - The response from the LLM.
 */
async function chatWithLLM(message) {
    try {
        const response = await ollama.default.chat({
            model: 'hf.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF:Q6_K',
            messages: [{ role: 'user', content: message }]
        });
        if (!response.message || !response.message.content) {
            throw new Error('Invalid response from LLM');
        }
        return response.message.content;
    } catch (error) {
        console.error('Error interacting with LLM:', error);
        throw error;
    }
}

/**
 * Uses the LLM to generate code based on a prompt.
 * @param {string} prompt - The prompt to send to the LLM.
 * @returns {Promise<string>} - The generated code from the LLM.
 */
async function generateCode(prompt) {
    try {
        const response = await ollama.default.generate({
            model: 'hf.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF:Q6_K',
            prompt: prompt
        });
        if (!response.message || !response.message.content) {
            throw new Error('Invalid response from LLM');
        }
        return response.message.content;
    } catch (error) {
        console.error('Error generating code with LLM:', error);
        return 'Error generating code.';
    }
}

/**
 * Uses the LLM to analyze the system state and provide feedback.
 * @param {Object} state - The current system state.
 * @returns {Promise<string>} - The feedback from the LLM.
 */
async function analyzeState(state) {
    try {
        const message = `Analyze the following state: ${JSON.stringify(state)}`;
        const response = await ollama.default.chat({
            model: 'hf.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF:Q6_K',
            messages: [{ role: 'user', content: message }]
        });
        if (!response.message || !response.message.content) {
            throw new Error('Invalid response from LLM');
        }
        return response.message.content;
    } catch (error) {
        console.error('Error analyzing state:', error);
        throw error;
    }
}

/**
 * Analyzes the README.md content and generates an execution plan.
 * @param {string} readmeContent - The content of the README.md file.
 * @param {Object} systemState - The current system state.
 * @returns {Promise<Object>} - The execution plan generated by the LLM.
 */
async function processUserIntent(readmeContent, systemState) {
    try {
        const message = `Analyze the following README content and generate an execution plan: ${readmeContent}`;
        const context = {
            systemState: systemState,
            availableFunctions: Object.keys(systemState.functions),
            availableWarmholes: Object.keys(systemState.warmholes)
        };
        const response = await ollama.default.chat({
            model: 'hf.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF:Q6_K',
            messages: [{ role: 'user', content: JSON.stringify({ message, context }) }]
        });
        if (!response.message || !response.message.content) {
            throw new Error('Invalid response from LLM');
        }
        let plan;
        try {
            plan = JSON.parse(response.message.content);
        } catch (error) {
            console.error('Received plain text response from LLM:', response.message.content);
            throw new Error('LLM response is not valid JSON');
        }
        if (!plan.steps) {
            throw new Error('Invalid plan format from LLM');
        }
        return plan;
    } catch (error) {
        console.error('Error processing user intent:', error);
        throw error;
    }
}

module.exports = {
    chatWithLLM,
    generateCode,
    analyzeState,
    processUserIntent
};
